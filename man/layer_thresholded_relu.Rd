% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/layers.R
\name{layer_thresholded_relu}
\alias{layer_thresholded_relu}
\title{Thresholded Rectified Linear Unit}
\usage{
layer_thresholded_relu(object, theta = 1, ...)
}
\arguments{
\item{theta}{Float >= 0. Threshold location of activation.}

\item{...}{standard layer arguments.}
}
\description{
Thresholded Rectified Linear Unit
}
\details{
It follows:

\if{html}{\out{<div class="sourceCode">}}\preformatted{    f(x) = x for x > theta
    f(x) = 0 otherwise`
}\if{html}{\out{</div>}}

Input shape:
Arbitrary. Use the keyword argument \code{input_shape}
(list of integers, does not include the samples axis)
when using this layer as the first layer in a model.

Output shape:
Same shape as the input.
}
\seealso{
\itemize{
\item \url{https://keras.io/api/layers}
}
}
