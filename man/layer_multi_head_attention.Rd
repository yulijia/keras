% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/layers.R
\name{layer_multi_head_attention}
\alias{layer_multi_head_attention}
\title{MultiHeadAttention layer}
\usage{
layer_multi_head_attention(
  inputs,
  num_heads,
  key_dim,
  value_dim = NULL,
  dropout = 0,
  use_bias = TRUE,
  output_shape = NULL,
  attention_axes = NULL,
  kernel_initializer = "glorot_uniform",
  bias_initializer = "zeros",
  kernel_regularizer = NULL,
  bias_regularizer = NULL,
  activity_regularizer = NULL,
  kernel_constraint = NULL,
  bias_constraint = NULL,
  ...
)
}
\arguments{
\item{num_heads}{Number of attention heads.}

\item{key_dim}{Size of each attention head for query and key.}

\item{value_dim}{Size of each attention head for value.}

\item{dropout}{Dropout probability.}

\item{use_bias}{Boolean, whether the dense layers use bias vectors/matrices.}

\item{output_shape}{The expected shape of an output tensor, besides the batch
and sequence dims. If not specified, projects back to the query
feature dim (the query input's last dimension).}

\item{attention_axes}{axes over which the attention is applied. \code{NULL} means
attention over all axes, but batch, heads, and features.}

\item{kernel_initializer}{Initializer for dense layer kernels.}

\item{bias_initializer}{Initializer for dense layer biases.}

\item{kernel_regularizer}{Regularizer for dense layer kernels.}

\item{bias_regularizer}{Regularizer for dense layer biases.}

\item{activity_regularizer}{Regularizer for dense layer activity.}

\item{kernel_constraint}{Constraint for dense layer kernels.}

\item{bias_constraint}{Constraint for dense layer kernels.}

\item{...}{standard layer arguments.}
}
\description{
MultiHeadAttention layer
}
\details{
This is an implementation of multi-headed attention as described in the
paper "Attention is all you Need" (Vaswani et al., 2017).
If \code{query}, \verb{key,} \code{value} are the same, then
this is self-attention. Each timestep in \code{query} attends to the
corresponding sequence in \code{key}, and returns a fixed-width vector.

This layer first projects \code{query}, \code{key} and \code{value}. These are
(effectively) a list of tensors of length \code{num_attention_heads}, where the
corresponding shapes are \verb{(batch_size, <query dimensions>, key_dim)},
\verb{(batch_size, <key/value dimensions>, key_dim)},
\verb{(batch_size, <key/value dimensions>, value_dim)}.

Then, the query and key tensors are dot-producted and scaled. These are
softmaxed to obtain attention probabilities. The value tensors are then
interpolated by these probabilities, then concatenated back to a single
tensor.

Finally, the result tensor with the last dimension as value_dim can take an
linear projection and return.

When using \code{MultiHeadAttention} inside a custom layer, the custom layer must
implement its own \code{build()} method and call \code{MultiHeadAttention}'s
\verb{_build_from_signature()} there.
This enables weights to be restored correctly when the model is loaded.
}
\seealso{
\itemize{
\item \url{https://keras.io/api/layers}
}
}
