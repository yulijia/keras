% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/layers.R
\name{layer_activation_selu}
\alias{layer_activation_selu}
\title{Scaled Exponential Linear Unit (SELU)}
\usage{
layer_activation_selu(object, ...)
}
\arguments{
\item{...}{
  Arguments passed on to \code{\link[=layer_activation]{layer_activation}}
  \describe{
    \item{\code{activation}}{Activation function, such as \code{tf.nn.relu}, or string name of
built-in activation function, such as "relu".}
  }}

\item{x}{A tensor or variable to compute the activation function for.}
}
\description{
Scaled Exponential Linear Unit (SELU)
}
\details{
The Scaled Exponential Linear Unit (SELU) activation function is defined as:
\itemize{
\item \verb{if x > 0: return scale * x}
\item \verb{if x < 0: return scale * alpha * (exp(x) - 1)}
}

where \code{alpha} and \code{scale} are pre-defined constants
(\code{alpha=1.67326324} and \code{scale=1.05070098}).

Basically, the SELU activation function multiplies \code{scale} (> 1) with the
output of the \code{tf.keras.activations.elu} function to ensure a slope larger
than one for positive inputs.

The values of \code{alpha} and \code{scale} are
chosen so that the mean and variance of the inputs are preserved
between two consecutive layers as long as the weights are initialized
correctly (see \code{tf.keras.initializers.LecunNormal} initializer)
and the number of input units is "large enough"
(see reference paper for more information).

Example Usage:

\if{html}{\out{<div class="sourceCode python">}}\preformatted{>>> num_classes = 10  # 10-class problem
>>> model = tf.keras.Sequential()
>>> model.add(tf.keras.layers.Dense(64, kernel_initializer='lecun_normal',
...                                 activation='selu'))
>>> model.add(tf.keras.layers.Dense(32, kernel_initializer='lecun_normal',
...                                 activation='selu'))
>>> model.add(tf.keras.layers.Dense(16, kernel_initializer='lecun_normal',
...                                 activation='selu'))
>>> model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))
}\if{html}{\out{</div>}}

The scaled exponential unit activation: \code{scale * elu(x, alpha)}.

Notes:
- To be used together with the
\code{tf.keras.initializers.LecunNormal} initializer.
- To be used together with the dropout variant
\code{tf.keras.layers.AlphaDropout} (not regular dropout).

References:
- \href{https://arxiv.org/abs/1706.02515}{Klambauer et al., 2017}
}
\seealso{
\itemize{
\item \url{https://keras.io/api/layers}
}
}
