% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/zzzz.R
\name{layer_alpha_dropout}
\alias{layer_alpha_dropout}
\title{Applies Alpha Dropout to the input}
\usage{
layer_alpha_dropout(object, rate, noise_shape = NULL, seed = NULL, ...)
}
\arguments{
\item{rate}{float, drop probability (as with \code{Dropout}).
The multiplicative noise will have
standard deviation \code{sqrt(rate / (1 - rate))}.}

\item{seed}{Integer, optional random seed to enable deterministic behavior.}

\item{...}{standard layer arguments.}
}
\description{
Applies Alpha Dropout to the input
}
\details{
Alpha Dropout is a \code{Dropout} that keeps mean and variance of inputs
to their original values, in order to ensure the self-normalizing property
even after this dropout.
Alpha Dropout fits well to Scaled Exponential Linear Units
by randomly setting activations to the negative saturation value.
}
\seealso{
\itemize{
\item \url{https://keras.io/api/layers}
}
}
