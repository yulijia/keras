% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/layers.R
\name{layer_p_relu}
\alias{layer_p_relu}
\title{Parametric Rectified Linear Unit}
\usage{
layer_p_relu(
  object,
  alpha_initializer = "zeros",
  alpha_regularizer = NULL,
  alpha_constraint = NULL,
  shared_axes = NULL,
  ...
)
}
\arguments{
\item{alpha_initializer}{Initializer function for the weights.}

\item{alpha_regularizer}{Regularizer for the weights.}

\item{alpha_constraint}{Constraint for the weights.}

\item{shared_axes}{The axes along which to share learnable
parameters for the activation function.
For example, if the incoming feature maps
are from a 2D convolution
with output shape \verb{(batch, height, width, channels)},
and you wish to share parameters across space
so that each filter only has one set of parameters,
set \verb{shared_axes=[1, 2]}.}

\item{...}{standard layer arguments.}
}
\description{
Parametric Rectified Linear Unit
}
\details{
It follows:

\if{html}{\out{<div class="sourceCode">}}\preformatted{    f(x) = alpha * x for x < 0
    f(x) = x for x >= 0
}\if{html}{\out{</div>}}

where \code{alpha} is a learned array with the same shape as x.

Input shape:
Arbitrary. Use the keyword argument \code{input_shape}
(list of integers, does not include the samples axis)
when using this layer as the first layer in a model.

Output shape:
Same shape as the input.
}
\seealso{
\itemize{
\item \url{https://keras.io/api/layers}
}
}
