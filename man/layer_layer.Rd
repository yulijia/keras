% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/layers.R
\name{layer_layer}
\alias{layer_layer}
\title{This is the class from which all layers inherit}
\usage{
layer_layer(object, ...)
}
\arguments{
\item{...}{standard layer arguments.}

\item{dynamic}{Set this to \code{TRUE} if your layer should only be run eagerly, and
should not be used to generate a static computation graph.
This would be the case for a Tree-RNN or a recursive network,
for example, or generally for any layer that manipulates tensors
using Python control flow. If \code{FALSE}, we assume that the layer can
safely be used to generate a static computation graph.}

\item{variable_dtype}{Alias of \code{dtype}.}

\item{compute_dtype}{The dtype of the layer's computations. Layers automatically
cast inputs to this dtype which causes the computations and output to
also be in this dtype. When mixed precision is used with a
\code{tf.keras.mixed_precision.Policy}, this will be different than
\code{variable_dtype}.}

\item{dtype_policy}{The layer's dtype policy. See the
\code{tf.keras.mixed_precision.Policy} documentation for details.}

\item{trainable_weights}{List of variables to be included in backprop.}

\item{non_trainable_weights}{List of variables that should not be
included in backprop.}

\item{weights}{The concatenation of the lists trainable_weights and
non_trainable_weights (in this order).}

\item{trainable}{Whether the layer should be trained (boolean), i.e. whether
its potentially-trainable weights should be returned as part of
\code{layer.trainable_weights}.}

\item{input_spec}{Optional (list of) \code{InputSpec} object(s) specifying the
constraints on inputs that can be accepted by the layer.}
}
\description{
This is the class from which all layers inherit
}
\details{
A layer is a callable object that takes as input one or more tensors and
that outputs one or more tensors. It involves \emph{computation}, defined
in the \code{call()} method, and a \emph{state} (weight variables). State can be
created in various places, at the convenience of the subclass implementer:
\itemize{
\item in \verb{__init__()};
\item in the optional \code{build()} method, which is invoked by the first
\verb{__call__()} to the layer, and supplies the shape(s) of the input(s),
which may not have been known at initialization time;
\item in the first invocation of \code{call()}, with some caveats discussed
below.
}

Layers are recursively composable: If you assign a Layer instance as an
attribute of another Layer, the outer layer will start tracking the weights
created by the inner layer. Nested layers should be instantiated in the
\verb{__init__()} method.

Users will just instantiate a layer and then treat it as a callable.

We recommend that descendants of \code{Layer} implement the following methods:
\itemize{
\item \verb{__init__()}: Defines custom layer attributes, and creates layer weights
that do not depend on input shapes, using \code{add_weight()}, or other state.
\item \code{build(self, input_shape)}: This method can be used to create weights that
depend on the shape(s) of the input(s), using \code{add_weight()}, or other
state. \verb{__call__()} will automatically build the layer (if it has not been
built yet) by calling \code{build()}.
\item \verb{call(self, inputs, *args, **kwargs)}: Called in \verb{__call__} after making
sure \code{build()} has been called. \code{call()} performs the logic of applying
the layer to the \code{inputs}. The first invocation may additionally create
state that could not be conveniently created in \code{build()}; see its
docstring for details.
Two reserved keyword arguments you can optionally use in \code{call()} are:
\itemize{
\item \code{training} (boolean, whether the call is in inference mode or training
mode). See more details in \href{https://www.tensorflow.org/guide/keras/custom_layers_and_models#privileged_training_argument_in_the_call_method}{the layer/model subclassing guide}
\item \code{mask} (boolean tensor encoding masked timesteps in the input, used
in RNN layers). See more details in
\href{https://www.tensorflow.org/guide/keras/custom_layers_and_models#privileged_mask_argument_in_the_call_method}{the layer/model subclassing guide}
A typical signature for this method is \code{call(self, inputs)}, and user
could optionally add \code{training} and \code{mask} if the layer need them. \verb{*args}
and \verb{**kwargs} is only useful for future extension when more input
parameters are planned to be added.
}
\item \code{get_config(self)}: Returns a dictionary containing the configuration used
to initialize this layer. If the keys differ from the arguments
in \verb{__init__}, then override \code{from_config(self)} as well.
This method is used when saving
the layer or a model that contains this layer.
}

Here's a basic example: a layer with two variables, \code{w} and \code{b},
that returns \verb{y = w . x + b}.
It shows how to implement \code{build()} and \code{call()}.
Variables set as attributes of a layer are tracked as weights
of the layers (in \code{layer.weights}).

\if{html}{\out{<div class="sourceCode python">}}\preformatted{class SimpleDense(Layer):

  def __init__(self, units=32):
      super(SimpleDense, self).__init__()
      self.units = units

  def build(self, input_shape):  # Create the state of the layer (weights)
    w_init = tf.random_normal_initializer()
    self.w = tf.Variable(
        initial_value=w_init(shape=(input_shape[-1], self.units),
                             dtype='float32'),
        trainable=TRUE)
    b_init = tf.zeros_initializer()
    self.b = tf.Variable(
        initial_value=b_init(shape=(self.units,), dtype='float32'),
        trainable=TRUE)

  def call(self, inputs):  # Defines the computation from inputs to outputs
      return tf.matmul(inputs, self.w) + self.b

# Instantiates the layer.
linear_layer = SimpleDense(4)

# This will also call `build(input_shape)` and create the weights.
y = linear_layer(tf.ones((2, 2)))
assert len(linear_layer.weights) == 2

# These weights are trainable, so they're listed in `trainable_weights`:
assert len(linear_layer.trainable_weights) == 2
}\if{html}{\out{</div>}}

Note that the method \code{add_weight()} offers a shortcut to create weights:

\if{html}{\out{<div class="sourceCode python">}}\preformatted{class SimpleDense(Layer):

  def __init__(self, units=32):
      super(SimpleDense, self).__init__()
      self.units = units

  def build(self, input_shape):
      self.w = self.add_weight(shape=(input_shape[-1], self.units),
                               initializer='random_normal',
                               trainable=TRUE)
      self.b = self.add_weight(shape=(self.units,),
                               initializer='random_normal',
                               trainable=TRUE)

  def call(self, inputs):
      return tf.matmul(inputs, self.w) + self.b
}\if{html}{\out{</div>}}

Besides trainable weights, updated via backpropagation during training,
layers can also have non-trainable weights. These weights are meant to
be updated manually during \code{call()}. Here's a example layer that computes
the running sum of its inputs:

\if{html}{\out{<div class="sourceCode python">}}\preformatted{class ComputeSum(Layer):

  def __init__(self, input_dim):
      super(ComputeSum, self).__init__()
      # Create a non-trainable weight.
      self.total = tf.Variable(initial_value=tf.zeros((input_dim,)),
                               trainable=FALSE)

  def call(self, inputs):
      self.total.assign_add(tf.reduce_sum(inputs, axis=0))
      return self.total

my_sum = ComputeSum(2)
x = tf.ones((2, 2))

y = my_sum(x)
print(y.numpy())  # [2. 2.]

y = my_sum(x)
print(y.numpy())  # [4. 4.]

assert my_sum.weights == [my_sum.total]
assert my_sum.non_trainable_weights == [my_sum.total]
assert my_sum.trainable_weights == []
}\if{html}{\out{</div>}}

For more information about creating layers, see the guide
\href{https://www.tensorflow.org/guide/keras/custom_layers_and_models}{Making new Layers and Models via subclassing}
}
\seealso{
\itemize{
\item \url{https://keras.io/api/layers}
}
}
