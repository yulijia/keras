% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/layers.R
\name{layer_activation_relu}
\alias{layer_activation_relu}
\title{Rectified Linear Unit activation function}
\usage{
layer_activation_relu(
  object,
  max_value = NULL,
  negative_slope = 0,
  threshold = 0,
  ...
)
}
\arguments{
\item{max_value}{Float >= 0. Maximum activation value. NULL means unlimited.
Defaults to \code{NULL}.}

\item{negative_slope}{Float >= 0. Negative slope coefficient.
Defaults to \code{0.}.}

\item{threshold}{Float >= 0. Threshold value for thresholded activation.
Defaults to \code{0.}.}

\item{...}{standard layer arguments.}
}
\description{
Rectified Linear Unit activation function
}
\details{
With default values, it returns element-wise \code{max(x, 0)}.

Otherwise, it follows:

\if{html}{\out{<div class="sourceCode">}}\preformatted{    f(x) = max_value if x >= max_value
    f(x) = x if threshold <= x < max_value
    f(x) = negative_slope * (x - threshold) otherwise
}\if{html}{\out{</div>}}

Usage:

\if{html}{\out{<div class="sourceCode python">}}\preformatted{>>> layer = tf.keras.layers.ReLU()
>>> output = layer([-3.0, -1.0, 0.0, 2.0])
>>> list(output.numpy())
[0.0, 0.0, 0.0, 2.0]
>>> layer = tf.keras.layers.ReLU(max_value=1.0)
>>> output = layer([-3.0, -1.0, 0.0, 2.0])
>>> list(output.numpy())
[0.0, 0.0, 0.0, 1.0]
>>> layer = tf.keras.layers.ReLU(negative_slope=1.0)
>>> output = layer([-3.0, -1.0, 0.0, 2.0])
>>> list(output.numpy())
[-3.0, -1.0, 0.0, 2.0]
>>> layer = tf.keras.layers.ReLU(threshold=1.5)
>>> output = layer([-3.0, -1.0, 1.0, 2.0])
>>> list(output.numpy())
[0.0, 0.0, 0.0, 2.0]
}\if{html}{\out{</div>}}

Input shape:
Arbitrary. Use the keyword argument \code{input_shape}
(list of integers, does not include the batch axis)
when using this layer as the first layer in a model.

Output shape:
Same shape as the input.
}
\seealso{
\itemize{
\item \url{https://keras.io/api/layers}
}
}
