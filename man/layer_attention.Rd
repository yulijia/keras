% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/layers.R
\name{layer_attention}
\alias{layer_attention}
\title{Dot-product attention layer, a.k.a. Luong-style attention}
\usage{
layer_attention(object, use_scale = FALSE, score_mode = "dot", ...)
}
\arguments{
\item{use_scale}{If \code{TRUE}, will create a scalar variable to scale the
attention scores.}

\item{score_mode}{Function to use to compute attention scores, one of
\verb{\{"dot", "concat"\}}. \code{"dot"} refers to the dot product between the
query and key vectors. \code{"concat"} refers to the hyperbolic tangent
of the concatenation of the query and key vectors.}

\item{...}{standard layer arguments.}

\item{dropout}{Float between 0 and 1. Fraction of the units to drop for the
attention scores. Defaults to 0.0.}
}
\description{
Dot-product attention layer, a.k.a. Luong-style attention
}
\details{
Inputs are \code{query} tensor of shape \verb{[batch_size, Tq, dim]}, \code{value} tensor
of shape \verb{[batch_size, Tv, dim]} and \code{key} tensor of shape
\verb{[batch_size, Tv, dim]}. The calculation follows the steps:
\enumerate{
\item Calculate scores with shape \verb{[batch_size, Tq, Tv]} as a \code{query}-\code{key} dot
product: \code{scores = tf.matmul(query, key, transpose_b=TRUE)}.
\item Use scores to calculate a distribution with shape
\verb{[batch_size, Tq, Tv]}: \code{distribution = tf.nn.softmax(scores)}.
\item Use \code{distribution} to create a linear combination of \code{value} with
shape \verb{[batch_size, Tq, dim]}:
\verb{return tf.matmul(distribution, value)}.
}
}
\seealso{
\itemize{
\item \url{https://keras.io/api/layers}
}
}
