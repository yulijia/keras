% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/zzzz.R
\name{layer_leaky_relu}
\alias{layer_leaky_relu}
\title{Leaky version of a Rectified Linear Unit}
\usage{
layer_leaky_relu(object, alpha = 0.3, ...)
}
\arguments{
\item{alpha}{Float >= \code{0.}. Negative slope coefficient. Defaults to \code{0.3}.}

\item{...}{standard layer arguments.}
}
\description{
Leaky version of a Rectified Linear Unit
}
\details{
It allows a small gradient when the unit is not active:

\if{html}{\out{<div class="sourceCode">}}\preformatted{    f(x) = alpha * x if x < 0
    f(x) = x if x >= 0
}\if{html}{\out{</div>}}

Usage:

\if{html}{\out{<div class="sourceCode python">}}\preformatted{>>> layer = tf.keras.layers.LeakyReLU()
>>> output = layer([-3.0, -1.0, 0.0, 2.0])
>>> list(output.numpy())
[-0.9, -0.3, 0.0, 2.0]
>>> layer = tf.keras.layers.LeakyReLU(alpha=0.1)
>>> output = layer([-3.0, -1.0, 0.0, 2.0])
>>> list(output.numpy())
[-0.3, -0.1, 0.0, 2.0]
}\if{html}{\out{</div>}}

Input shape:
Arbitrary. Use the keyword argument \code{input_shape}
(list of integers, does not include the batch axis)
when using this layer as the first layer in a model.

Output shape:
Same shape as the input.
}
\seealso{
\itemize{
\item \url{https://keras.io/api/layers}
}
}
